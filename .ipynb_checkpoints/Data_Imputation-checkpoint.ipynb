{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Imputation - How to Deal with Missing Data\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Refs:\n",
    "- https://www.kaggle.com/residentmario/simple-techniques-for-missing-data-imputation\n",
    "- https://scikit-learn.org/stable/modules/impute.html\n",
    "- https://towardsdatascience.com/how-to-handle-missing-data-8646b18db0d4\n",
    "- https://www.residentmar.io/2016/06/12/null-and-missing-data-python.html\n",
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Missing data is a well-known problem in many data-related fields. Almost in every single project which contains data, in early stages of it we need to face the famous problem of \"Dealing with Missing Data\". This project is not an exception, so we have to face this problem, just before we move on to preparing our model. as you can see in the \"Ads\" sheet of our dataset, we have some missing data which we are going to solve in this notebook.\n",
    "\n",
    "It's worth to mention that we are going to test different methods for this problem and choose the one which works the best for this case."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most machine learning algorithms (except KNN) cannot deal with this problem intrinsically, as they are designed for complete data. so we have to do something regarding this matter.\n",
    "\n",
    "Missing data can be categorized into two types:\n",
    "- Data missing at random\n",
    "- Data missing at not random\n",
    "\n",
    "There are different approaches regarding the category of missing data.\n",
    "\n",
    ">If the data are truly NMAR, then the missing data mechanism must be modeled as part of the estimation process in order to produce unbiased parameter estimates. That means that, if there is missing data on Y, one must specify how the probability that Y is missing depends on Y and on other variables. This is not straightforward because there are an infinite number of different models that one could specify. Nothing in the data will indicate which of these models is correct. And, unfortunately, results could be highly sensitive to the choice of model. A good deal of research has been devoted to the problem of data that are not missing at random, and some progress has been made. Unfortunately, the available methods are rather complex, even for very simple situations.\n",
    "\n",
    "\"Handling Missing Data by Maximum Likelihood\"—Paul D. Allison, Statistical Horizons, Haverford, PA, USA\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple Approaches\n",
    "\n",
    "#### Dropping rows with null values\n",
    "\n",
    "The easiest and quickest approach to a missing data problem is dropping the offending entries. This is an acceptable solution if we are confident that the missing data in the dataset is missing at random, and if the number of data points we have access to is sufficiently high that dropping some of them will not cause us to lose generalizability in the models we build (to determine whether or not this is case, use a learning curve).\n",
    "\n",
    "Dropping data missing not at random is dangerous. It will result in significant bias in your model in cases where data being absent corresponds with some real-world phenomenon. Because this requires domain knowledge, usually the only way to determine if this is a problem is through manual inspection.\n",
    "\n",
    "Dropping too much data is also dangerous. It can create significant bias by depriving your algorithms of space. This is especially true of classifiers sensitive to the curse of dimensionality.\n",
    "\n",
    "#### Dropping features with high nullity\n",
    "\n",
    "A feature that has a high number of empty values is unlikely to be very useful for prediction. It can often be safely dropped.\n",
    "Dropping rare features simplifies your model, but obviously gives you fewer features to work with. Before dropping features outright, consider subsetting the part of the dataset that this value is available for and checking its feature importance when it is used to train a model in this subset. If in doing so you disover that the variable is important in the subset it is defined, consider making an effort to retain it.\n",
    "\n",
    "#### Mean or median or other summary statistic substitution\n",
    "\n",
    "The remainder of the techniques available are imputation methods, as opposed to data-dropping methods. The simplest imputation method is replacing missing values with the mean or median values of the dataset at large, or some similar summary statistic. This has the advantage of being the simplest possible approach, and one that doesn't introduce any undue bias into the dataset. But:\n",
    "\n",
    "\n",
    "> [However] with missing values that are not strictly random, especially in the presence of a great inequality in the number of missing values for the different variables, the mean substitution method may lead to inconsistent bias. Furthermore, this approach adds no new information but only increases the sample size and leads to an underestimate of the errors. Thus, mean substitution is not generally accepted.\n",
    "\n",
    "Kang H. (2013). The prevention and handling of the missing data. Korean journal of anesthesiology, 64(5), 402–406. https://doi.org/10.4097/kjae.2013.64.5.402 \n",
    "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3668100/\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Imputation\n",
    "\n",
    "Here's a fun trick. To prepare a dataset for machine learning we need to fix missing values, and we can fix missing values by applying machine learning to that dataset! If we consider a column with missing data as our target variable, and existing columns with complete data as our predictor variables, then we may construct a machine learning model using complete records as our train and test datasets and the records with incomplete data as our generalization target. This is a fully scoped-out machine learning problem.\n",
    "\n",
    "I'm going to implement this method since simple approaches is not really suitable for this problem, since droping column-wise or row-wise will terminate almost most of the database and no valuable information could be extracted from this data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>post_link</th>\n",
       "      <th>caption</th>\n",
       "      <th>user_id</th>\n",
       "      <th>followers_current</th>\n",
       "      <th>view</th>\n",
       "      <th>like</th>\n",
       "      <th>comment</th>\n",
       "      <th>save</th>\n",
       "      <th>impression</th>\n",
       "      <th>reach</th>\n",
       "      <th>engagement</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>BX3IiPYHOaN</td>\n",
       "      <td>@BeKhatereMan\\n.\\nاگر شما هم نگران عزیزانتان ...</td>\n",
       "      <td>akharinkhabar</td>\n",
       "      <td>992000.0</td>\n",
       "      <td>21489</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>@BeKhatereMan\\n.\\nاگر شما هم نگران عزیزانتان ...</td>\n",
       "      <td>3kansbartar</td>\n",
       "      <td>1502221.0</td>\n",
       "      <td>39495</td>\n",
       "      <td>6809.0</td>\n",
       "      <td>36.0</td>\n",
       "      <td>352.0</td>\n",
       "      <td>162000.0</td>\n",
       "      <td>129000.0</td>\n",
       "      <td>7197.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>@BeKhatereMan\\n.\\nاگر شما هم نگران عزیزانتان ...</td>\n",
       "      <td>bache_khordani</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5838</td>\n",
       "      <td>794.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>61.0</td>\n",
       "      <td>30100.0</td>\n",
       "      <td>23300.0</td>\n",
       "      <td>862.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>@BeKhatereMan\\n.\\nاگر شما هم نگران عزیزانتان ...</td>\n",
       "      <td>ajibtar_az_elm</td>\n",
       "      <td>636403.0</td>\n",
       "      <td>14327</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>@BeKhatereMan\\n.\\nاگر شما هم نگران عزیزانتان ...</td>\n",
       "      <td>banoo.page</td>\n",
       "      <td>1476998.0</td>\n",
       "      <td>10305</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   index    post_link                                            caption  \\\n",
       "0      1  BX3IiPYHOaN   @BeKhatereMan\\n.\\nاگر شما هم نگران عزیزانتان ...   \n",
       "1      2          NaN   @BeKhatereMan\\n.\\nاگر شما هم نگران عزیزانتان ...   \n",
       "2      3          NaN   @BeKhatereMan\\n.\\nاگر شما هم نگران عزیزانتان ...   \n",
       "3      4          NaN   @BeKhatereMan\\n.\\nاگر شما هم نگران عزیزانتان ...   \n",
       "4      5          NaN   @BeKhatereMan\\n.\\nاگر شما هم نگران عزیزانتان ...   \n",
       "\n",
       "          user_id  followers_current   view    like  comment   save  \\\n",
       "0   akharinkhabar           992000.0  21489     NaN      NaN    NaN   \n",
       "1     3kansbartar          1502221.0  39495  6809.0     36.0  352.0   \n",
       "2  bache_khordani                NaN   5838   794.0      7.0   61.0   \n",
       "3  ajibtar_az_elm           636403.0  14327     NaN      NaN    NaN   \n",
       "4      banoo.page          1476998.0  10305     NaN      NaN    NaN   \n",
       "\n",
       "   impression     reach  engagement  \n",
       "0         NaN       NaN         NaN  \n",
       "1    162000.0  129000.0      7197.0  \n",
       "2     30100.0   23300.0       862.0  \n",
       "3         NaN       NaN         NaN  \n",
       "4         NaN       NaN         NaN  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xls = pd.ExcelFile('data/msc_thesis_dataset.xlsx')\n",
    "df = pd.read_excel(xls, 'Ads')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 28 records with empty features in this dataset.\n"
     ]
    }
   ],
   "source": [
    "null_records_count = len(df[(df['like'].isna()) & (df['comment'].isna()) & (df['save'].isna()) & (df['impression'].isna()) & (df['reach'].isna()) & (df['engagement'].isna())])\n",
    "print(f'There are {null_records_count} records with empty features in this dataset.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
